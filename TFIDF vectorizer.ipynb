{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/13632/Downloads/dataset_stance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    " \n",
    "def lemmatizing(data):\n",
    "    lemmatized_data = []\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i]\n",
    "\n",
    "        # tokenize the sentence and find the POS tag for each token\n",
    "        word_tokens = nltk.wordpunct_tokenize(sentence) #what does wordpunct_tokenize do?\n",
    "        if w in word_tokens in stopwords.words('english'):\n",
    "            del w\n",
    "        pos_tagged = nltk.pos_tag(word_tokens) \n",
    "\n",
    "        \n",
    "        # As you may have noticed, the above pos tags are a little confusing.\n",
    "        \n",
    "        # we use our own pos_tagger function to make things simpler to understand.\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "        \n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                # if there is no available tag, append the token as is\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:       \n",
    "                # else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "        lemmatized_data.append(lemmatized_sentence)\n",
    "    return lemmatized_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider how to count punctuation too. are they considered by the vectorizer function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidfVectorizer\n",
    "def tfidf_vectorizer(list):\n",
    "    vectorizer = TfidfTransformer(lowercase = False. ngram_range(1,3))\n",
    "    matrix = vectorizer.fit_transform()\n",
    "    idf_table = pd.DataFrame(matrix[0].T.todense())\n",
    "    return idf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N words with highest IDF\n",
    "def n_top_words(count_table, n):\n",
    "    count_table_sorted = sorted(count_table, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words = count_table_sorted[:n]\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
