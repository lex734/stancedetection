{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/13632/Downloads/dataset_stance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: HTTP Error 401:\n",
      "[nltk_data]     Unauthorized\n",
      "[nltk_data] Error loading stopwords: HTTP Error 401: Unauthorized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))) - skipping\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Directory 'C:/Users/13632/AppData/Roaming/nltk_data/corpora/averaged_perceptron_tagger' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install \"C:/Users/13632/AppData/Roaming/nltk_data/corpora/averaged_perceptron_tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Directory 'C:/Users/13632/AppData/Roaming/nltk_data/corpora/stopwords' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))) - skipping\n"
     ]
    }
   ],
   "source": [
    "pip install \"C:/Users/13632/AppData/Roaming/nltk_data/corpora/stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    " \n",
    "def lemmatizing(data):\n",
    "    lemmatized_data = []\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i]\n",
    "\n",
    "        # tokenize the sentence and find the POS tag for each token\n",
    "        word_tokens = nltk.wordpunct_tokenize(sentence) #what does wordpunct_tokenize do?\n",
    "        pos_tagged = nltk.pos_tag(word_tokens) \n",
    "\n",
    "        \n",
    "        # As you may have noticed, the above pos tags are a little confusing.\n",
    "        \n",
    "        # we use our own pos_tagger function to make things simpler to understand.\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "        \n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                # if there is no available tag, append the token as is\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:       \n",
    "                # else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "        lemmatized_data.append(lemmatized_sentence)\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no separation based on subtopic\n",
    "def grouping(df):\n",
    "    df_for = []\n",
    "    df_against = []\n",
    "    df_neutral = []\n",
    "    for k in range(len(df)):\n",
    "        if df['Stance'][k] == 'FAVOR':\n",
    "            df_for.append(df['Tweet'][k]) \n",
    "        elif df['Stance'][k] == 'AGAINST':\n",
    "            df_against.append(df['Tweet'][k])\n",
    "        else:\n",
    "            df_neutral.append(df['Tweet'][k])\n",
    "    return df_for, df_against, df_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['Target'])\n",
    "df_atheism = grouped_df.get_group('Atheism')\n",
    "df_climate = grouped_df.get_group('Climate Change is a Real Concern')\n",
    "df_trump = grouped_df.get_group('Donald Trump')\n",
    "df_feminist = grouped_df.get_group('Feminist Movement')\n",
    "df_clinton = grouped_df.get_group('Hillary Clinton')\n",
    "df_abortion = grouped_df.get_group('Legalization of Abortion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider how to count punctuation too. are they considered by the vectorizer function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidfVectorizer\n",
    "def tfidf_vectorizer(list):\n",
    "    list = [x.lower() for x in list]\n",
    "    list = lemmatizing(list)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "    matrix = vectorizer.fit_transform(list)\n",
    "    idf_table = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return idf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tfidf_vectorizer(df_atheism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>opinion towards</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "      <th>target</th>\n",
       "      <th>towards</th>\n",
       "      <th>tweet</th>\n",
       "      <th>unnamed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   opinion  opinion towards  sentiment  stance  target  towards  tweet  \\\n",
       "0  0.00000          0.00000        0.0     0.0     0.0  0.00000    0.0   \n",
       "1  0.00000          0.00000        0.0     0.0     0.0  0.00000    1.0   \n",
       "2  0.00000          0.00000        0.0     0.0     1.0  0.00000    0.0   \n",
       "3  0.00000          0.00000        0.0     1.0     0.0  0.00000    0.0   \n",
       "4  0.57735          0.57735        0.0     0.0     0.0  0.57735    0.0   \n",
       "5  0.00000          0.00000        1.0     0.0     0.0  0.00000    0.0   \n",
       "\n",
       "   unnamed  \n",
       "0      1.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "5      0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE N words with highest IDF\n",
    "def n_top_words(count_table, n):\n",
    "    count_table_sorted = sorted(count_table, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words = count_table_sorted[:n]\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
